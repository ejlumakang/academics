{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxceS8Bd/1arTPSq1ol8a+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unigram"
      ],
      "metadata": {
        "id": "IWYMQGs38PJ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LA7vzm96oz9",
        "outputId": "68c77d42-c37a-48b0-c5f9-257a770f5db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of the sentence \"the an incorporated\" is: 0.00024601254663987864\n",
            "\n",
            "Word Probabilities:\n",
            "fifth: 0.034482758620689655\n",
            "an: 0.06896551724137931\n",
            "of: 0.034482758620689655\n",
            "futures: 0.034482758620689655\n",
            "the: 0.10344827586206896\n",
            "incorporated: 0.034482758620689655\n",
            "a: 0.06896551724137931\n",
            "inflation: 0.034482758620689655\n",
            "most: 0.034482758620689655\n",
            "dollars: 0.034482758620689655\n",
            "quarter: 0.034482758620689655\n",
            "in: 0.034482758620689655\n",
            "is: 0.034482758620689655\n",
            "mass: 0.034482758620689655\n",
            "thrift: 0.034482758620689655\n",
            "did: 0.034482758620689655\n",
            "eighty: 0.034482758620689655\n",
            "said: 0.034482758620689655\n",
            "hard: 0.034482758620689655\n",
            "m: 0.034482758620689655\n",
            "july: 0.034482758620689655\n",
            "bullish: 0.034482758620689655\n",
            "that: 0.034482758620689655\n",
            "or: 0.034482758620689655\n",
            "limited: 0.034482758620689655\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Sample list of words (the corpus)\n",
        "words = [\n",
        "    'fifth','an','of','futures','the','an','incorporated','a',\n",
        "    'a','the','inflation','most','dollars','quarter','in','is',\n",
        "    'mass','thrift','did','eighty','said','hard','m','july','bullish',\n",
        "    'that','or','limited','the'\n",
        "]\n",
        "\n",
        "# Function to build a Unigram model\n",
        "def build_unigram_model(words):\n",
        "    word_count = defaultdict(int)\n",
        "    total_count = len(words)\n",
        "    for word in words:\n",
        "        word_count[word] += 1\n",
        "    # Calculate probabilities\n",
        "    probabilities = {word: count / total_count for word, count in word_count.items()}\n",
        "    return probabilities\n",
        "\n",
        "# Function to estimate the probability of a sentence\n",
        "def estimate_sentence_probability(sentence, probabilities):\n",
        "    words = sentence.split()\n",
        "    prob = 1.0\n",
        "    for word in words:\n",
        "        prob *= probabilities.get(word, 0)\n",
        "    return prob\n",
        "\n",
        "# Build the Unigram model\n",
        "unigram_model = build_unigram_model(words)\n",
        "\n",
        "# Test sentence\n",
        "test_sentence = 'the an incorporated'\n",
        "probability = estimate_sentence_probability(test_sentence, unigram_model)\n",
        "\n",
        "print(f'The probability of the sentence \"{test_sentence}\" is: {probability}')\n",
        "\n",
        "# Display individual word probabilities\n",
        "print('\\nWord Probabilities:')\n",
        "for word, prob in unigram_model.items():\n",
        "    print(f'{word}: {prob}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Sample list of words (the corpus)\n",
        "words = [\n",
        "    'the', 'quick', 'brown', 'fox',\n",
        "    'jumps', 'over', 'the', 'lazy', 'dog',\n",
        "    'the', 'dog', 'jumps', 'over', 'the', 'fence'\n",
        "]\n",
        "\n",
        "# Function to build a Unigram model\n",
        "def build_unigram_model(words):\n",
        "    word_count = defaultdict(int)\n",
        "    total_count = len(words)\n",
        "    for word in words:\n",
        "        word_count[word] += 1\n",
        "    # Calculate probabilities\n",
        "    probabilities = {word: count / total_count for word, count in word_count.items()}\n",
        "    return probabilities\n",
        "\n",
        "# Function to estimate the probability of a sentence\n",
        "def estimate_sentence_probability(sentence, probabilities):\n",
        "    words = sentence.split()\n",
        "    prob = 1.0\n",
        "    for word in words:\n",
        "        prob *= probabilities.get(word, 0)\n",
        "    return prob\n",
        "\n",
        "# Build the Unigram model\n",
        "unigram_model = build_unigram_model(words)\n",
        "\n",
        "# Test sentence\n",
        "test_sentence = 'the quick'\n",
        "probability = estimate_sentence_probability(test_sentence, unigram_model)\n",
        "\n",
        "print(f'The probability of the sentence \"{test_sentence}\" is: {probability}')\n",
        "\n",
        "# Display individual word probabilities\n",
        "print('\\nWord Probabilities:')\n",
        "for word, prob in unigram_model.items():\n",
        "    print(f'{word}: {prob}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2bnJbtYUCIK",
        "outputId": "2b8d6013-8be0-4b84-b5df-a9b460ee0c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of the sentence \"the quick\" is: 0.017777777777777778\n",
            "\n",
            "Word Probabilities:\n",
            "the: 0.26666666666666666\n",
            "quick: 0.06666666666666667\n",
            "brown: 0.06666666666666667\n",
            "fox: 0.06666666666666667\n",
            "jumps: 0.13333333333333333\n",
            "over: 0.13333333333333333\n",
            "lazy: 0.06666666666666667\n",
            "dog: 0.13333333333333333\n",
            "fence: 0.06666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram"
      ],
      "metadata": {
        "id": "DOI_AnmN8S-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Sample list of words (the corpus)\n",
        "# Sample list of words (the corpus)\n",
        "words = [\n",
        "    'fifth','an','of','futures','the','an','incorporated','a',\n",
        "    'a','the','inflation','most','dollars','quarter','in','is',\n",
        "    'mass','thrift','did','eighty','said','hard','m','july','bullish',\n",
        "    'that','or','limited','the'\n",
        "]\n",
        "\n",
        "# Function to build a Bigram model\n",
        "def build_bigram_model(words):\n",
        "  bigram_count = defaultdict(lambda: defaultdict(int))\n",
        "  unigram_count = defaultdict(int)\n",
        "\n",
        "  for i in range(len(words) - 1):\n",
        "    first_word = words[i]\n",
        "    second_word = words[i + 1]\n",
        "    bigram_count[first_word][second_word] += 1\n",
        "    unigram_count[first_word] += 1\n",
        "\n",
        "  # Calculate probabilities\n",
        "  bigram_probabilities = {\n",
        "      first_word: {second_word: count / unigram_count[first_word]\n",
        "                   for second_word, count in second_word_counts.items()}\n",
        "      for first_word, second_word_counts in bigram_count.items()\n",
        "  }\n",
        "  return bigram_probabilities\n",
        "\n",
        "# Function to predict the next word\n",
        "def predict_next_word(previous_word, bigram_probabilities):\n",
        "  if previous_word in bigram_probabilities:\n",
        "    return max(bigram_probabilities[previous_word], key=bigram_probabilities[previous_word].get)\n",
        "  else:\n",
        "    return None # No prediction available\n",
        "\n",
        "# Build the Bigram model\n",
        "bigram_model = build_bigram_model(words)\n",
        "\n",
        "# Test the prediction\n",
        "previous_word = 'in'\n",
        "predicted_word = predict_next_word(previous_word, bigram_model)\n",
        "\n",
        "print(f\"Given the word '{previous_word}', the predicted next word is: '{predicted_word}'\")\n",
        "\n",
        "# Display bigram probabilities\n",
        "print('\\nBigram Probabilities:')\n",
        "for first_word, second_words in bigram_model.items():\n",
        "  for second_word, prob in second_words.items():\n",
        "    print(f\"P({second_word} | {first_word}) = {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq-q9GC68Sbn",
        "outputId": "a714bed3-96d8-49ea-e437-86cabd643a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given the word 'in', the predicted next word is: 'is'\n",
            "\n",
            "Bigram Probabilities:\n",
            "P(an | fifth) = 1.0000\n",
            "P(of | an) = 0.5000\n",
            "P(incorporated | an) = 0.5000\n",
            "P(futures | of) = 1.0000\n",
            "P(the | futures) = 1.0000\n",
            "P(an | the) = 0.5000\n",
            "P(inflation | the) = 0.5000\n",
            "P(a | incorporated) = 1.0000\n",
            "P(a | a) = 0.5000\n",
            "P(the | a) = 0.5000\n",
            "P(most | inflation) = 1.0000\n",
            "P(dollars | most) = 1.0000\n",
            "P(quarter | dollars) = 1.0000\n",
            "P(in | quarter) = 1.0000\n",
            "P(is | in) = 1.0000\n",
            "P(mass | is) = 1.0000\n",
            "P(thrift | mass) = 1.0000\n",
            "P(did | thrift) = 1.0000\n",
            "P(eighty | did) = 1.0000\n",
            "P(said | eighty) = 1.0000\n",
            "P(hard | said) = 1.0000\n",
            "P(m | hard) = 1.0000\n",
            "P(july | m) = 1.0000\n",
            "P(bullish | july) = 1.0000\n",
            "P(that | bullish) = 1.0000\n",
            "P(or | that) = 1.0000\n",
            "P(limited | or) = 1.0000\n",
            "P(the | limited) = 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Sample list of words (the corpus)\n",
        "words = [\n",
        "    'the', 'quick', 'brown', 'fox',\n",
        "    'jumps', 'over', 'the', 'lazy', 'dog',\n",
        "    'the', 'dog', 'jumps', 'over', 'the', 'fence'\n",
        "]\n",
        "\n",
        "# Function to build a Bigram model\n",
        "def build_bigram_model(words):\n",
        "  bigram_count = defaultdict(lambda: defaultdict(int))\n",
        "  unigram_count = defaultdict(int)\n",
        "\n",
        "  for i in range(len(words) - 1):\n",
        "    first_word = words[i]\n",
        "    second_word = words[i + 1]\n",
        "    bigram_count[first_word][second_word] += 1\n",
        "    unigram_count[first_word] += 1\n",
        "\n",
        "  # Calculate probabilities\n",
        "  bigram_probabilities = {\n",
        "      first_word: {second_word: count / unigram_count[first_word]\n",
        "                   for second_word, count in second_word_counts.items()}\n",
        "      for first_word, second_word_counts in bigram_count.items()\n",
        "  }\n",
        "  return bigram_probabilities\n",
        "\n",
        "# Function to predict the next word\n",
        "def predict_next_word(previous_word, bigram_probabilities):\n",
        "  if previous_word in bigram_probabilities:\n",
        "    return max(bigram_probabilities[previous_word],\n",
        "               key=bigram_probabilities[previous_word].get)\n",
        "  else:\n",
        "    return None # No prediction available\n",
        "\n",
        "# Build the Bigram model\n",
        "bigram_model = build_bigram_model(words)\n",
        "\n",
        "# Test the prediction\n",
        "previous_word = 'brown'\n",
        "predicted_word = predict_next_word(previous_word, bigram_model)\n",
        "\n",
        "print(f\"Given the word '{previous_word}', the predicted next word is: '{predicted_word}'\")\n",
        "\n",
        "# Display bigram probabilities\n",
        "print('\\nBigram Probabilities:')\n",
        "for first_word, second_words in bigram_model.items():\n",
        "  for second_word, prob in second_words.items():\n",
        "    print(f\"P({second_word} | {first_word}) = {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFReqT7FfDYF",
        "outputId": "1fb00e42-cd89-4291-c55b-48649c2f8489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given the word 'brown', the predicted next word is: 'fox'\n",
            "\n",
            "Bigram Probabilities:\n",
            "P(quick | the) = 0.2500\n",
            "P(lazy | the) = 0.2500\n",
            "P(dog | the) = 0.2500\n",
            "P(fence | the) = 0.2500\n",
            "P(brown | quick) = 1.0000\n",
            "P(fox | brown) = 1.0000\n",
            "P(jumps | fox) = 1.0000\n",
            "P(over | jumps) = 1.0000\n",
            "P(the | over) = 1.0000\n",
            "P(dog | lazy) = 1.0000\n",
            "P(the | dog) = 0.5000\n",
            "P(jumps | dog) = 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram"
      ],
      "metadata": {
        "id": "IbzVRCuI8Z2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample sentence\n",
        "sentence = 'the cat sat on the mat'\n",
        "\n",
        "# Function to generate N-grams\n",
        "def generate_ngrams(words, n):\n",
        "  ngrams = []\n",
        "  for i in range(len(words) - n + 1):\n",
        "    ngrams.append(tuple(words[i:i+n]))\n",
        "  return ngrams\n",
        "\n",
        "# Function to build N-gram model\n",
        "def build_ngram_model(words, n):\n",
        "  ngram_count = defaultdict(int)\n",
        "  for ngram in generate_ngrams(words, n):\n",
        "    ngram_count[ngram] += 1\n",
        "\n",
        "  total_ngrams = sum(ngram_count.values())\n",
        "  ngram_probabilities = {ngram: count / total_ngrams for ngram, count in ngram_count.items()}\n",
        "  return ngram_probabilities\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Build N-gram models\n",
        "unigram_model = build_ngram_model(words, 1)\n",
        "bigram_model = build_ngram_model(words, 2)\n",
        "trigram_model = build_ngram_model(words, 3)\n",
        "\n",
        "# Display N-gram probabilities\n",
        "print('Unigram Probabilities:')\n",
        "for unigram, prob in unigram_model.items():\n",
        "  print(f'P({unigram}) = {prob:.4f}')\n",
        "\n",
        "print('\\nBigram Probabilities:')\n",
        "for bigram, prob in bigram_model.items():\n",
        "  print(f'P({bigram}) = {prob:.4f}')\n",
        "\n",
        "print('\\nTrigram Probabilities:')\n",
        "for trigram, prob in trigram_model.items():\n",
        "  print(f'P({trigram}) = {prob:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvkxl6iu8ZaB",
        "outputId": "4ee1a8f7-796a-4ecc-c5a2-b43c8c9ee518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Probabilities:\n",
            "P(('the',)) = 0.3333\n",
            "P(('cat',)) = 0.1667\n",
            "P(('sat',)) = 0.1667\n",
            "P(('on',)) = 0.1667\n",
            "P(('mat',)) = 0.1667\n",
            "\n",
            "Bigram Probabilities:\n",
            "P(('the', 'cat')) = 0.2000\n",
            "P(('cat', 'sat')) = 0.2000\n",
            "P(('sat', 'on')) = 0.2000\n",
            "P(('on', 'the')) = 0.2000\n",
            "P(('the', 'mat')) = 0.2000\n",
            "\n",
            "Trigram Probabilities:\n",
            "P(('the', 'cat', 'sat')) = 0.2500\n",
            "P(('cat', 'sat', 'on')) = 0.2500\n",
            "P(('sat', 'on', 'the')) = 0.2500\n",
            "P(('on', 'the', 'mat')) = 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Keywords"
      ],
      "metadata": {
        "id": "Rb40BxJ0Ajty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# Sample sentences from the Berkeley Restaurant Project\n",
        "sentences = [\n",
        "    \"Can you tell me about any good Cantonese restaurants close by?\",\n",
        "    \"Mid-priced Thai food is what I'm looking for.\",\n",
        "    \"Tell me about Chez Panisse.\",\n",
        "    \"Can you give me a listing of the kinds of food that are available?\",\n",
        "    \"I'm looking for a good place to eat breakfast.\",\n",
        "    \"When is Caffe Venezia open during the day?\"\n",
        "]\n",
        "\n",
        "# Function to count keywords\n",
        "def count_keywords(sentences):\n",
        "  keywords = ['restaurant','food','breakfast','lunch','dinner','cuisine','menu']\n",
        "  keyword_count = defaultdict(int)\n",
        "\n",
        "  for sentence in sentences:\n",
        "    # Normalize the sentence to lowercase and remove punctuation\n",
        "    cleaned_sentence = re.sub(r'[^\\w\\s]','',sentence.lower())\n",
        "    words = cleaned_sentence.split()\n",
        "\n",
        "    for word in words:\n",
        "      if word in keywords:\n",
        "        keyword_count[word] += 1\n",
        "\n",
        "  return keyword_count\n",
        "\n",
        "# Count keywords\n",
        "keyword_counts = count_keywords(sentences)\n",
        "\n",
        "# Display keyword counts\n",
        "for keyword, count in keyword_counts.items():\n",
        "  print(f'{keyword}: {count}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBrEMpAxAjPQ",
        "outputId": "5774d8a0-04b4-4944-981f-022118c991d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "food: 2\n",
            "breakfast: 1\n"
          ]
        }
      ]
    }
  ]
}