{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUxNvvrzMvJHL7qReMsN1F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rxaYm8_rfJXO",
        "outputId": "129b7dd8-8332-46a1-ce44-ec6970605374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: spacy\n",
            "Version: 3.7.5\n",
            "Summary: Industrial-strength Natural Language Processing (NLP) in Python\n",
            "Home-page: https://spacy.io\n",
            "Author: Explosion\n",
            "Author-email: contact@explosion.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\n",
            "Required-by: en-core-web-sm, fastai\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from nltk.tokenize import word_tokenize  # tokenization: breakdown of text into individual worrds\n",
        "from nltk.corpus import stopwords        # stopwords: common words (is, and, the)\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# download nltk resources\n",
        "nltk.download('punkt')  # tokenizer model used for splitting text into sentences (sentence tokenization)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load spacy model\n",
        "nlp = English()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "W2W74zothfOC",
        "outputId": "49a144ff-0c9d-4d5b-fc75-ca2591b1153e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk\n",
        "text = \"NLP is fascinating field of Artifical Intelligence\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens using NLTK: \", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUYML58Qjash",
        "outputId": "cb92de7f-95e0-4bf6-ee93-f63f21fe2bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens using NLTK:  ['NLP', 'is', 'fascinating', 'field', 'of', 'Artifical', 'Intelligence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(\"Tokens using SpaCy: \", spacy_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HulCnQFsj8qY",
        "outputId": "ec1d1cbd-a268-425d-f55d-33327f06dbaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens using SpaCy:  ['NLP', 'is', 'fascinating', 'field', 'of', 'Artifical', 'Intelligence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# countvectorizer: tool that converts a collection of text docu into matrix of tokens -> tokenize and counts how often each word appears in text\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# multinomial: naive bayes classifier for multinomially distribute data, often used for text classification - represents freq or counts\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# make_pipeline: used to create a pipeline that sequentially combine several processing steps into a single objects\n",
        "\n",
        "# example\n",
        "texts = [\n",
        "    \"I love programming\", \"Programming is great\", \"I really enjoy solving problems\",\n",
        "    \"I hate bugs\", \"Bugs are frustrating\", \"I dislike errors\", \"I cannot stand issues\",\n",
        "    \"Coding is so much fun\", \"I find programming to be amazing\", \"I dislike debugging\",\n",
        "    \"I adore my teacher\"\n",
        "    ]\n",
        "labels = [\"Positive\", \"Positive\", \"Positive\", \"Negative\", \"Negative\",\n",
        "          \"Negative\", \"Negative\", \"Positive\", \"Positive\", \"Negative\", \"Neutral\"\n",
        "          ]\n",
        "\n",
        "# pipeline for the classification\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "model.fit(texts, labels)\n",
        "\n",
        "# user input\n",
        "user_input = input(\"Enter a sentence for sentiment analysis: \")\n",
        "\n",
        "# prediction from the user input\n",
        "prediction = model.predict([user_input])\n",
        "print(\"Predicted sentiment: \", prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk5U0woAk1Ns",
        "outputId": "6ab9c26c-4f51-406f-9e59-f1b575a13d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence for sentiment analysis: i adore my teacher\n",
            "Predicted sentiment:  ['Neutral']\n"
          ]
        }
      ]
    }
  ]
}