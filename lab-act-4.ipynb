{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqybuigBdJRZlIMGil/EZw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Tokenizing Customer Feedback"
      ],
      "metadata": {
        "id": "Cgt8QhQaYTuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.\tTokenize the Feedback into Sentences and Words using NLTK"
      ],
      "metadata": {
        "id": "LaQiF5JKZFR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8PDOg0EAZDjP",
        "outputId": "32948831-7d5c-4cca-9047-f1c15da53460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"Great product, but the software crashed twice in the last week. The customer support team was very helpful, though. Could improve the battery life.\"\n",
        "\n",
        "# Tokenization\n",
        "def tokenize_sentences(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "def tokenize_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    return words\n",
        "\n",
        "# Sentences\n",
        "sentences = tokenize_sentences(text)\n",
        "print(sentences)\n",
        "\n",
        "# Words\n",
        "words = tokenize_words(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOyyL8XZaRKj",
        "outputId": "4b512909-7f26-4900-f9d2-59cc83879156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Great product, but the software crashed twice in the last week.', 'The customer support team was very helpful, though.', 'Could improve the battery life.']\n",
            "['Great', 'product', ',', 'but', 'the', 'software', 'crashed', 'twice', 'in', 'the', 'last', 'week', '.', 'The', 'customer', 'support', 'team', 'was', 'very', 'helpful', ',', 'though', '.', 'Could', 'improve', 'the', 'battery', 'life', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.\tTokenize the Feedback into Words using spaCy"
      ],
      "metadata": {
        "id": "luIBclOBZLVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spacy model\n",
        "nlp = spacy.blank('en')\n",
        "\n",
        "# Process text using spacy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenization\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtxRWI3-ZOSr",
        "outputId": "2f293a9f-82f3-4665-bb59-ab5807733fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Great', 'product', ',', 'but', 'the', 'software', 'crashed', 'twice', 'in', 'the', 'last', 'week', '.', 'The', 'customer', 'support', 'team', 'was', 'very', 'helpful', ',', 'though', '.', 'Could', 'improve', 'the', 'battery', 'life', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Removing Stopwords"
      ],
      "metadata": {
        "id": "I3aZkO_ccO4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.\tRemove Stopwords using NLTK"
      ],
      "metadata": {
        "id": "fuoaeUm2dZVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "removed_tokens = [token for token in tokens if token.lower() in stop_words]\n",
        "\n",
        "print(filtered_tokens)\n",
        "print(removed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ49uzkvct-A",
        "outputId": "da365eb0-6a7f-4724-8da8-f71999338ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Great', 'product', ',', 'software', 'crashed', 'twice', 'last', 'week', '.', 'customer', 'support', 'team', 'helpful', ',', 'though', '.', 'Could', 'improve', 'battery', 'life', '.']\n",
            "['but', 'the', 'in', 'the', 'The', 'was', 'very', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Remove Stopwords using spaCy"
      ],
      "metadata": {
        "id": "EFOTBS1Odda2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spacy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process text using spacy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "removed_tokens = [token.text for token in doc if token.is_stop]\n",
        "\n",
        "print(filtered_tokens)\n",
        "print(removed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqRScbYYcTwy",
        "outputId": "c6faefd7-3d36-4227-e269-6a2add953171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Great', 'product', ',', 'software', 'crashed', 'twice', 'week', '.', 'customer', 'support', 'team', 'helpful', ',', '.', 'improve', 'battery', 'life', '.']\n",
            "['but', 'the', 'in', 'the', 'last', 'The', 'was', 'very', 'though', 'Could', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Extracting Named Entities"
      ],
      "metadata": {
        "id": "rDYpN6iQfKyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Extract Named Entities Using spaCy"
      ],
      "metadata": {
        "id": "6BZeQDQAgiF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spacy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process text using spacy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extracting named entities\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WhUhZg5hD1K",
        "outputId": "472c7d95-dbd7-4eea-db26-e147f8c9fcf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the last week DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Evaluating and Reflecting on Tokenization"
      ],
      "metadata": {
        "id": "kFMelLyggpbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the sample customer feedback input, the tokenization results is quite the same for NLTK and Spacy. Both do their jobs to split texts into words, and considers punctuations as separate tokens. Due to the simplicity of the input, I think both methods provided accurate results. However, I consider NLTK as the one easier to use simply because it is straightforward unlike Spacy which used specific OOP data structures like the doc object."
      ],
      "metadata": {
        "id": "IdsufaALmLmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Both NLTK and Spacy provide different text processing functionalities. NLTK is flexible and rule-based, but it doesn't have a built-in pipeline unlike Spacy. On the other hand, Spacy is fast and efficient but lacks the flexibility compared to NLTK.\n",
        "\n",
        "- Tokenization help with analyzing customer feedback because it is part of the preprocessing technique, which transforms unstructured data into a simplified form so that we can extract relevant insights.\n",
        "\n",
        "- Removing stopwords impact the analysis by reducing the noise, allowing us to focus on relevant words and reduce the size of the data to process. It also improves the accuracy and performance of models in many NLP tasks.\n",
        "\n",
        "- It is important to extract named entities from customer feedback because it can be a way to identify customer insights to products, market competitions, and emerging trends. These data are then used as basis to come up with the next course of action in terms of businesses.\n",
        "\n",
        "- Frequent terms or keywords should be looked to understand the context of the insights of the customer, along with their expressed sentiments to the product or service, whether it is positive or negative. And most importantly, the named entities to identify what to maintain and improve regarding the product or service."
      ],
      "metadata": {
        "id": "OxUARLcqpwQY"
      }
    }
  ]
}